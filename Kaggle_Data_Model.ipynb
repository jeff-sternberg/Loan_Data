{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "             Copy from Local To HDFS\n",
    "(If data is in relational db,use sqoop to import)=========>\n",
    "                                                          Create & Load data in Hive=======>\n",
    "                                                                            PYSpark jobs to Transform & load back\n",
    "                                                                                 to hive leveraging HDFS storage\n",
    "\n",
    "\n",
    "#I am going to put my Data Model/Schema in Hive(wont be able to run queries here but i am putting up \n",
    "#the code for creating & loading the necessary data for this system\").I am going to drop few fields & using only fields\n",
    "#which can give me some kind of informative data\n",
    "                                            \n",
    "#Columns for my Schema\n",
    "#(id,loan_amnt,term,int_rate,emp_title,issue_d,loan_status,purpose,addr_state,total_pymnt,application_type)\n",
    "\n",
    "#Step1:\n",
    "First, create a Hdfs directory Loan_Data and copy the loan.csv file in the HDFS system in warehouse directory which is the default location for\n",
    "creating any tables in Hive.\n",
    "    hadoop fs -mkdir Loan_Data\n",
    "    hadoop fs -copyFromLocal Users/Downloads/loan.csv /warehouse/Loan_Data/loan.csv\n",
    "\n",
    "#Step 2\n",
    "#Create the Schema\n",
    "\n",
    "CREATE DATABASE IF NOT EXISTS LOAN;\n",
    "USE LOAN;\n",
    "\n",
    "#Step 3\n",
    "#Create Hive Tables partitioned by term\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS Loan_data (\n",
    "id string PRIMARY KEY NOT NULL,\n",
    "loan_amnt float,\n",
    "term string,\n",
    "int_rate float,\n",
    "emp_title string,\n",
    "issue_d string,\n",
    "loan_status string,\n",
    "purpose string,\n",
    "addr_state string,\n",
    "total_pymnt float,\n",
    "application_type string)\n",
    "PARTITIONED BY (period STRING)\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t';\n",
    "\n",
    "#Step 4\n",
    "#Check if table is created succesfully\n",
    "DESCRIBE FORMATTED Loan_data;\n",
    "\n",
    "#Step 5 \n",
    "#Load Data to Hive for each partition \n",
    "FROM Loan_data l\n",
    "INSERT OVERWRITE TABLE Loan_data\n",
    "PARTITION (period = '36 months')\n",
    "SELECT * WHERE l.term = '36 months'\n",
    "INSERT OVERWRITE TABLE Loan_data\n",
    "PARTITION (period = '60 months')\n",
    "SELECT * WHERE l.term = '60 months';\n",
    "\n",
    "#Steps 6\n",
    "#Check if partitions are created succesfully\n",
    "SHOW PARTITIONS Loan_data;\n",
    "\n",
    "#Step 7\n",
    "#TRAVERSE PARTITIONS IN HDFS\n",
    "!hadoop fs -ls /user/hive/warehouse/Loan.Loan_data/period=36 months;\n",
    "!hadoop fs -ls /user/hive/warehouse/Loan.Loan_data/period=60 months;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
