{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      term|count|\n",
      "+----------+-----+\n",
      "| 36 months|   45|\n",
      "| 60 months|   11|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Question 1 Solution\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.appName('abc').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "#Reading the csv file(I could have read the same file from hdfs, but it wont work in this setup, so im reading file)\n",
    "data =spark.read.csv(\"\\\\Users\\\\PoonamYadav\\\\Downloads\\\\lending-club-loan-data\\\\new_sample_loan.csv\",header=True)\n",
    "\n",
    "#Registering table as Dataframe\n",
    "table = sqlContext.registerDataFrameAsTable(data, \"Loan_table\")\n",
    "\n",
    "#Selecting good_loan as‘Current’, ‘Issued’ and ‘Fully Paid’ \n",
    "good_loan= spark.sql(\"SELECT term,loan_status as good_loan, ROW_NUMBER() OVER (PARTITION BY term ORDER BY term)as rn from Loan_table WHERE (loan_status = 'Current' or loan_status = 'Fully Paid' or loan_status = 'Issued')\")\n",
    "\n",
    "\n",
    "\n",
    "rs =good_loan.groupby(\"term\").count().show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solution of Question two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employee who is most loan is : (4, 'Starbucks')\n",
      "Employee who is least loan is : ('\"Gen-Probe', 1)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions\n",
    "\n",
    "\n",
    "# Create a SparkSession (the config bit is only for Windows!)\n",
    "spark = SparkSession.builder.config(\"spark.sql.warehouse.dir\", \"file:///C:/temp\").appName(\"Loan_data\").getOrCreate()\n",
    "\n",
    "\n",
    "# Get the raw data\n",
    "lines = spark.sparkContext.textFile(\"\\\\Users\\\\PoonamYadav\\\\Downloads\\\\lending-club-loan-data\\\\sample_loan.csv\")\n",
    "\n",
    "# Convert it to a RDD of Row objects\n",
    "Emp = lines.map(lambda x: (x.split(\",\")[5]))\n",
    "\n",
    "# Map it it x,1\n",
    "Empdata = Emp.map(lambda x: (x,1))\n",
    "emp_by_key = Empdata.reduceByKey(lambda x, y: x+y)\n",
    "\n",
    "#Finding the max emp_title\n",
    "max_sortByKey = emp_by_key.map(lambda x: (x[1],x[0])).sortByKey(ascending= False)\n",
    "max_results = max_sortByKey.first()\n",
    "print(\"Employee who took most loan is : {}\".format(max_results))\n",
    "\n",
    "#Finding the least emp_title\n",
    "min_sortByKey = emp_by_key.sortByKey()\n",
    "min_results = min_sortByKey.first()\n",
    "print(\"Employee who took least loan is : {}\".format(min_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "| loan_status|           purpose|\n",
      "+------------+------------------+\n",
      "| loan_status|           purpose|\n",
      "| Charged Off|               car|\n",
      "| Charged Off|    small_business|\n",
      "| Charged Off|             other|\n",
      "| Charged Off|debt_consolidation|\n",
      "| Charged Off|             other|\n",
      "| Charged Off|debt_consolidation|\n",
      "| Charged Off|    major_purchase|\n",
      "| Charged Off|debt_consolidation|\n",
      "| Charged Off|             other|\n",
      "|       Inc.\"|        Fully Paid|\n",
      "| Charged Off|debt_consolidation|\n",
      "| Charged Off|debt_consolidation|\n",
      "| Charged Off|               car|\n",
      "| Charged Off|debt_consolidation|\n",
      "| Charged Off|debt_consolidation|\n",
      "| Charged Off|debt_consolidation|\n",
      "|        Inc\"|        Fully Paid|\n",
      "| Charged Off|    small_business|\n",
      "| California\"|       Charged Off|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Solution of question3 \n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "spark = SparkSession.builder.appName('abc').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "def mapper(line):\n",
    "    fields = line.split(',')\n",
    "    return Row(loan_status=str(fields[2]), purpose=str(fields[3]))\n",
    "\n",
    "lines = sc.textFile(\"\\\\Users\\\\PoonamYadav\\\\Downloads\\\\lending-club-loan-data\\\\new_sample_loan.csv\")\n",
    "Loan_table = lines.map(mapper)\n",
    "#Registering table as Dataframe\n",
    "schemaPeople = spark.createDataFrame(Loan_table).cache()\n",
    "schemaPeople.createOrReplaceTempView(\"Loan_table\")\n",
    "\n",
    "\n",
    "#Selecting good_loan as‘Current’, ‘Issued’ and ‘Fully Paid’ \n",
    "bad_loan= spark.sql(\"SELECT loan_status, purpose from Loan_table WHERE ((loan_status <> 'Current')  and (loan_status <> 'Fully Paid') and (loan_status <> 'Issued'))\")\n",
    "\n",
    "bad_loan.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+\n",
      "|           purpose|cnt|\n",
      "+------------------+---+\n",
      "|        Fully Paid|  2|\n",
      "|           purpose|  1|\n",
      "|           wedding|  1|\n",
      "|             other|  8|\n",
      "|    small_business|  5|\n",
      "|       Charged Off|  1|\n",
      "|debt_consolidation| 34|\n",
      "|       credit_card| 14|\n",
      "|            moving|  1|\n",
      "|               car|  3|\n",
      "|    major_purchase|  2|\n",
      "|           medical|  1|\n",
      "|  home_improvement|  1|\n",
      "+------------------+---+\n",
      "\n",
      "+------------------+---+\n",
      "|           purpose|cnt|\n",
      "+------------------+---+\n",
      "|           purpose|  1|\n",
      "|               car|  3|\n",
      "|    small_business|  5|\n",
      "|             other|  8|\n",
      "|debt_consolidation| 34|\n",
      "|             other|  8|\n",
      "|debt_consolidation| 34|\n",
      "|    major_purchase|  2|\n",
      "|debt_consolidation| 34|\n",
      "|             other|  8|\n",
      "|        Fully Paid|  2|\n",
      "|debt_consolidation| 34|\n",
      "|debt_consolidation| 34|\n",
      "|               car|  3|\n",
      "|debt_consolidation| 34|\n",
      "|debt_consolidation| 34|\n",
      "|debt_consolidation| 34|\n",
      "|        Fully Paid|  2|\n",
      "|    small_business|  5|\n",
      "|       Charged Off|  1|\n",
      "+------------------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(purpose='debt_consolidation', cnt=34)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Solution of question3 Continued\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "spark = SparkSession.builder.appName('abc').getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "def mapper(line):\n",
    "    fields = line.split(',')\n",
    "    return Row(purpose=str(fields[3]))\n",
    "\n",
    "lines = sc.textFile(\"\\\\Users\\\\PoonamYadav\\\\Downloads\\\\lending-club-loan-data\\\\new_sample_loan.csv\")\n",
    "Purpose_table = lines.map(mapper)\n",
    "#Registering table as Dataframe\n",
    "schemaPeople = spark.createDataFrame(Purpose_table).cache()\n",
    "schemaPeople.createOrReplaceTempView(\"Purpose_table\")\n",
    "purpose_loan =spark.sql(\"SELECT purpose,count(*) as cnt from Purpose_table GROUP BY purpose\")\n",
    "purpose_loan.show()\n",
    "new_loan = bad_loan.join(purpose_loan, bad_loan.purpose == purpose_loan.purpose).select(bad_loan.purpose, purpose_loan.cnt)\n",
    "new_loan.show()\n",
    "new_loan.orderBy(\"cnt\", ascending=False).first()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
